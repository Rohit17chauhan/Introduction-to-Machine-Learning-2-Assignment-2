{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77245e5",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8535dc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Overfitting:\\nDefinition: Overfitting occurs when a model learns the training data too well, to the extent that it captures noise or random \\nfluctuations in the data rather than the underlying patterns. As a result, the model performs well on the training data but\\npoorly on unseen or test data.\\nConsequences: Overfitting leads to poor generalization, where the model fails to generalize its predictions to new, unseen data.\\nThis can result in inaccurate predictions and reduced performance in real-world scenarios.\\nMitigation:\\nRegularization: Techniques like L1 and L2 regularization add a penalty term to the loss function, discouraging overly complex\\nmodels.\\nCross-validation: Splitting the data into multiple subsets for training and testing helps evaluate the model's performance on \\nunseen data.\\nFeature selection/reduction: Removing irrelevant or redundant features can simplify the model and reduce overfitting.\\n\\nUnderfitting:\\nDefinition: Underfitting occurs when a model is too simple to capture the underlying structure of the data. It fails to learn \\nthe patterns in the training data and performs poorly on both the training and test datasets.\\nConsequences: Underfitting results in high bias and low variance, meaning the model is not able to capture the complexity of\\nthe data and therefore makes inaccurate predictions even on the training data.\\nMitigation:\\nIncreasing model complexity: Using more complex models with higher capacity, such as deep neural networks or ensembles, can help\\ncapture more intricate patterns in the data.\\nAdding features: Introducing additional relevant features or engineering more informative features can improve the model's \\nability to capture the underlying relationships in the data.\\nReducing regularization: In cases where regularization is too strong, reducing the regularization strength or removing it \\naltogether can help alleviate underfitting.\\nCollecting more data: Increasing the size of the training dataset can provide the model with more examples to learn from,\\npotentially capturing more complex patterns.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans1=\"\"\"Overfitting:\n",
    "Definition: Overfitting occurs when a model learns the training data too well, to the extent that it captures noise or random \n",
    "fluctuations in the data rather than the underlying patterns. As a result, the model performs well on the training data but\n",
    "poorly on unseen or test data.\n",
    "Consequences: Overfitting leads to poor generalization, where the model fails to generalize its predictions to new, unseen data.\n",
    "This can result in inaccurate predictions and reduced performance in real-world scenarios.\n",
    "Mitigation:\n",
    "Regularization: Techniques like L1 and L2 regularization add a penalty term to the loss function, discouraging overly complex\n",
    "models.\n",
    "Cross-validation: Splitting the data into multiple subsets for training and testing helps evaluate the model's performance on \n",
    "unseen data.\n",
    "Feature selection/reduction: Removing irrelevant or redundant features can simplify the model and reduce overfitting.\n",
    "\n",
    "Underfitting:\n",
    "Definition: Underfitting occurs when a model is too simple to capture the underlying structure of the data. It fails to learn \n",
    "the patterns in the training data and performs poorly on both the training and test datasets.\n",
    "Consequences: Underfitting results in high bias and low variance, meaning the model is not able to capture the complexity of\n",
    "the data and therefore makes inaccurate predictions even on the training data.\n",
    "Mitigation:\n",
    "Increasing model complexity: Using more complex models with higher capacity, such as deep neural networks or ensembles, can help\n",
    "capture more intricate patterns in the data.\n",
    "Adding features: Introducing additional relevant features or engineering more informative features can improve the model's \n",
    "ability to capture the underlying relationships in the data.\n",
    "Reducing regularization: In cases where regularization is too strong, reducing the regularization strength or removing it \n",
    "altogether can help alleviate underfitting.\n",
    "Collecting more data: Increasing the size of the training dataset can provide the model with more examples to learn from,\n",
    "potentially capturing more complex patterns.\"\"\"\n",
    "Ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee453d11",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4e2d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To reduce overfitting in machine learning models, consider the following techniques:\\n\\nRegularization: Add a penalty term to the loss function to discourage overly complex models. Common regularization techniques\\ninclude L1 and L2 regularization.\\nCross-validation: Split the data into multiple subsets for training and testing. Cross-validation helps evaluate the model's\\nperformance on unseen data and prevents overfitting to the training set.\\nFeature selection/reduction: Remove irrelevant or redundant features from the dataset to simplify the model and reduce \\noverfitting.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans2=\"\"\"To reduce overfitting in machine learning models, consider the following techniques:\n",
    "\n",
    "Regularization: Add a penalty term to the loss function to discourage overly complex models. Common regularization techniques\n",
    "include L1 and L2 regularization.\n",
    "Cross-validation: Split the data into multiple subsets for training and testing. Cross-validation helps evaluate the model's\n",
    "performance on unseen data and prevents overfitting to the training set.\n",
    "Feature selection/reduction: Remove irrelevant or redundant features from the dataset to simplify the model and reduce \n",
    "overfitting.\"\"\"\n",
    "Ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d6e1e",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf6c876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Underfitting occurs when a machine learning model is too simplistic to capture the underlying patterns in the data. \\nThis results in poor performance not only on the training data but also on unseen or test data. Underfitting often arises due \\nto the model's inability to represent the complexity of the underlying data distribut.\\n\\nHere are some scenarios where underfitting can occur in machine learning:\\n\\nLinear models on nonlinear data: Using linear regression or logistic regression models to fit nonlinear relationships in the \\ndata can lead to underfitting. These models have limited flexibility and may fail to capture the nonlinearities present in the\\ndata.\\nLimited training data: When the training dataset is small or unrepresentative of the underlying data distribution, it may not \\nprovide enough information for the model to learn the patterns effectively. This can lead to underfitting, where the model \\nfails to generalize well to new data.\\nIncorrect feature selection: If important features are omitted from the model or irrelevant features are included, the model\\nmay not capture the relevant information in the data, leading to underfitting.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans3=\"\"\"Underfitting occurs when a machine learning model is too simplistic to capture the underlying patterns in the data. \n",
    "This results in poor performance not only on the training data but also on unseen or test data. Underfitting often arises due \n",
    "to the model's inability to represent the complexity of the underlying data distribut.\n",
    "\n",
    "Here are some scenarios where underfitting can occur in machine learning:\n",
    "\n",
    "Linear models on nonlinear data: Using linear regression or logistic regression models to fit nonlinear relationships in the \n",
    "data can lead to underfitting. These models have limited flexibility and may fail to capture the nonlinearities present in the\n",
    "data.\n",
    "Limited training data: When the training dataset is small or unrepresentative of the underlying data distribution, it may not \n",
    "provide enough information for the model to learn the patterns effectively. This can lead to underfitting, where the model \n",
    "fails to generalize well to new data.\n",
    "Incorrect feature selection: If important features are omitted from the model or irrelevant features are included, the model\n",
    "may not capture the relevant information in the data, leading to underfitting.\"\"\"\n",
    "Ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747d4e6",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879887e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The bias-variance tradeoff is a fundamental concept in machine learning that relates to the balance between a model's bias and variance, and how it impacts the model's performance.\\n\\nBias: Bias refers to the error introduced by approximating a real-world problem with a simplified model. A high bias model \\nmakes strong assumptions about the underlying data and is likely to underfit, meaning it fails to capture the true relationships\\nbetween features and the target variable.\\nVariance: Variance refers to the model's sensitivity to fluctuations in the training data. A high variance model is overly\\ncomplex and captures noise in the training data, leading to overfitting. Such a model performs well on the training data but\\npoorly on unseen data.\\n\\nThe relationship between bias and variance can be summarized as follows:\\n\\nHigh Bias, Low Variance: Models with high bias and low variance are typically too simplistic and fail to capture the underlying \\npatterns in the data. They underfit the training data and perform poorly on both training and test datasets.\\nLow Bias, High Variance: Models with low bias and high variance are usually too complex and capture noise or random\\nfluctuations in the training data. They overfit the training data and perform well on the training dataset but poorly on unseen\\ndata due to their inability to generalize.\\n\\nThe goal in machine learning is to find the right balance between bias and variance to achieve good generalization performance \\non unseen data. This balance is crucial for building models that accurately capture the underlying patterns in the data without\\nbeing too simplistic or too complex.\\n\\nTechniques like regularization, cross-validation, and ensemble methods can help manage the bias-variance tradeoff by\\ncontrolling model complexity, evaluating model performance on unseen data, and combining multiple models to reduce variance. \\nUltimately, understanding and optimizing the bias-variance tradeoff is essential for developing machine learning models that\\ngeneralize well to new, unseen data.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans4=\"\"\"The bias-variance tradeoff is a fundamental concept in machine learning that relates to the balance between a model's bias and variance, and how it impacts the model's performance.\n",
    "\n",
    "Bias: Bias refers to the error introduced by approximating a real-world problem with a simplified model. A high bias model \n",
    "makes strong assumptions about the underlying data and is likely to underfit, meaning it fails to capture the true relationships\n",
    "between features and the target variable.\n",
    "Variance: Variance refers to the model's sensitivity to fluctuations in the training data. A high variance model is overly\n",
    "complex and captures noise in the training data, leading to overfitting. Such a model performs well on the training data but\n",
    "poorly on unseen data.\n",
    "\n",
    "The relationship between bias and variance can be summarized as follows:\n",
    "\n",
    "High Bias, Low Variance: Models with high bias and low variance are typically too simplistic and fail to capture the underlying \n",
    "patterns in the data. They underfit the training data and perform poorly on both training and test datasets.\n",
    "Low Bias, High Variance: Models with low bias and high variance are usually too complex and capture noise or random\n",
    "fluctuations in the training data. They overfit the training data and perform well on the training dataset but poorly on unseen\n",
    "data due to their inability to generalize.\n",
    "\n",
    "The goal in machine learning is to find the right balance between bias and variance to achieve good generalization performance \n",
    "on unseen data. This balance is crucial for building models that accurately capture the underlying patterns in the data without\n",
    "being too simplistic or too complex.\n",
    "\n",
    "Techniques like regularization, cross-validation, and ensemble methods can help manage the bias-variance tradeoff by\n",
    "controlling model complexity, evaluating model performance on unseen data, and combining multiple models to reduce variance. \n",
    "Ultimately, understanding and optimizing the bias-variance tradeoff is essential for developing machine learning models that\n",
    "generalize well to new, unseen data.\"\"\"\n",
    "Ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c9d4e",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c1497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"method for detecting overfitting and underfitting in machine learning models is through the analysis of the model's performance on a separate test dataset. Here's how:\\n\\nTraining and Testing Split:\\nSplit the dataset into three subsets: training, validation, and test sets. The training set is used to train the model, \\nthe validation set is used to tune hyperparameters and assess performance during training, and the test set is reserved for\\nfinal evaluation.\\n\\nPerformance Evaluation:\\nTrain the model on the training dataset and evaluate its performance on the validation set. Adjust hyperparameters or model \\ncomplexity based on the validation performance.\\nOnce the model is trained and fine-tuned, evaluate its performance on the test set. This provides an unbiased estimate of \\nthe model's performance on unseen data.\\n\\nDetecting Overfitting:\\nIf the model performs significantly better on the training set compared to the test set, it indicates overfitting. \\nThe model has memorized the training data but fails to generalize to new, unseen data.\\n\\nDetecting Underfitting:\\nIf the model performs poorly on both the training and test sets, it suggests underfitting. The model is too simplistic to \\ncapture the underlying patterns in the data.\\n\\nComparing Performance:\\nCompare the model's performance on the training, validation, and test sets. A large gap between training and test performance \\nsuggests overfitting, while poor performance on both training and test sets indicates underfitting.\\n\\nRepeating the Process:\\nIf overfitting or underfitting is detected, iterate on the model architecture, hyperparameters, or dataset preprocessing to\\naddress the issue.\\nEnsure that adjustments are based on validation performance to prevent overfitting to the test set.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans5=\"\"\"method for detecting overfitting and underfitting in machine learning models is through the analysis of the model's performance on a separate test dataset. Here's how:\n",
    "\n",
    "Training and Testing Split:\n",
    "Split the dataset into three subsets: training, validation, and test sets. The training set is used to train the model, \n",
    "the validation set is used to tune hyperparameters and assess performance during training, and the test set is reserved for\n",
    "final evaluation.\n",
    "\n",
    "Performance Evaluation:\n",
    "Train the model on the training dataset and evaluate its performance on the validation set. Adjust hyperparameters or model \n",
    "complexity based on the validation performance.\n",
    "Once the model is trained and fine-tuned, evaluate its performance on the test set. This provides an unbiased estimate of \n",
    "the model's performance on unseen data.\n",
    "\n",
    "Detecting Overfitting:\n",
    "If the model performs significantly better on the training set compared to the test set, it indicates overfitting. \n",
    "The model has memorized the training data but fails to generalize to new, unseen data.\n",
    "\n",
    "Detecting Underfitting:\n",
    "If the model performs poorly on both the training and test sets, it suggests underfitting. The model is too simplistic to \n",
    "capture the underlying patterns in the data.\n",
    "\n",
    "Comparing Performance:\n",
    "Compare the model's performance on the training, validation, and test sets. A large gap between training and test performance \n",
    "suggests overfitting, while poor performance on both training and test sets indicates underfitting.\n",
    "\n",
    "Repeating the Process:\n",
    "If overfitting or underfitting is detected, iterate on the model architecture, hyperparameters, or dataset preprocessing to\n",
    "address the issue.\n",
    "Ensure that adjustments are based on validation performance to prevent overfitting to the test set.\"\"\"\n",
    "Ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d28822",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a5ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bias:\\nBias refers to the error introduced by approximating a real-world problem with a simplified model.\\nHigh bias models are too simplistic and make strong assumptions about the underlying data.\\nThese models often underfit the training data, meaning they fail to capture the true relationships between features and the \\ntarget variable.\\nHigh bias models typically have low complexity and perform poorly on both training and test datasets.\\nVariance:\\n\\nVariance refers to the model's sensitivity to fluctuations in the training data.\\nHigh variance models are overly complex and capture noise or random fluctuations in the training data.\\nThese models often overfit the training data, meaning they perform well on the training data but poorly on unseen data due to \\ntheir inability to generalize.\\nHigh variance models typically have high complexity and perform well on the training data but poorly on the test data.\\n\\nExamples of high bias and high variance models:\\n\\nHigh Bias (Low Variance) Models:\\nLinear regression models with few features or low polynomial degrees are examples of high bias models. They make strong \\nassumptions about the linear relationship between features and the target variable.\\nDecision trees with shallow depths are also high bias models. They are too simplistic to capture complex decision boundaries \\nin the data.\\nThese models tend to underfit the data and have poor performance on both training and test datasets.\\n\\nHigh Variance (Low Bias) Models:\\nDeep neural networks with many layers and parameters are examples of high variance models. They have the capacity to capture \\ncomplex relationships in the data but are prone to overfitting.\\nRandom forests with a large number of decision trees are also high variance models. They can capture intricate patterns in \\nthe data but may suffer from overfitting if not properly regularized.\\nThese models tend to perform well on the training data but poorly on the test data due to their inability to generalize.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans6=\"\"\"Bias:\n",
    "Bias refers to the error introduced by approximating a real-world problem with a simplified model.\n",
    "High bias models are too simplistic and make strong assumptions about the underlying data.\n",
    "These models often underfit the training data, meaning they fail to capture the true relationships between features and the \n",
    "target variable.\n",
    "High bias models typically have low complexity and perform poorly on both training and test datasets.\n",
    "Variance:\n",
    "\n",
    "Variance refers to the model's sensitivity to fluctuations in the training data.\n",
    "High variance models are overly complex and capture noise or random fluctuations in the training data.\n",
    "These models often overfit the training data, meaning they perform well on the training data but poorly on unseen data due to \n",
    "their inability to generalize.\n",
    "High variance models typically have high complexity and perform well on the training data but poorly on the test data.\n",
    "\n",
    "Examples of high bias and high variance models:\n",
    "\n",
    "High Bias (Low Variance) Models:\n",
    "Linear regression models with few features or low polynomial degrees are examples of high bias models. They make strong \n",
    "assumptions about the linear relationship between features and the target variable.\n",
    "Decision trees with shallow depths are also high bias models. They are too simplistic to capture complex decision boundaries \n",
    "in the data.\n",
    "These models tend to underfit the data and have poor performance on both training and test datasets.\n",
    "\n",
    "High Variance (Low Bias) Models:\n",
    "Deep neural networks with many layers and parameters are examples of high variance models. They have the capacity to capture \n",
    "complex relationships in the data but are prone to overfitting.\n",
    "Random forests with a large number of decision trees are also high variance models. They can capture intricate patterns in \n",
    "the data but may suffer from overfitting if not properly regularized.\n",
    "These models tend to perform well on the training data but poorly on the test data due to their inability to generalize.\"\"\"\n",
    "Ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f136aea",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5affe317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Regularization in machine learning is a set of techniques used to prevent overfitting by adding a penalty term to the\\nmodel's loss function. The penalty term encourages the model to favor simpler solutions and reduces its tendency to fit noise\\nin the training data. Regularization helps improve the model's generalization ability, allowing it to perform well on unseen \\ndata.\\n\\nHere are some common regularization techniques and how they work:\\n\\nL1 Regularization (Lasso):\\nL1 regularization adds a penalty term proportional to the absolute value of the model's coefficients to the loss function.\\nThe penalty term encourages sparsity in the model, leading to feature selection by shrinking some coefficients to zero.\\nL1 regularization is effective for feature selection and building sparse models.\\n\\nL2 Regularization (Ridge):\\nL2 regularization adds a penalty term proportional to the square of the model's coefficients to the loss function.\\nThe penalty term penalizes large coefficients, effectively shrinking them towards zero.\\nL2 regularization encourages the model to distribute its weight more evenly across features, reducing the impact of individual \\nfeatures and preventing overfitting.\\n\\nEarly Stopping:\\nEarly stopping is a simple regularization technique that halts the training process when the model's performance on a \\nvalidation set starts to degrade.\\nBy preventing the model from training for too long and overfitting to the training data, early stopping helps improve \\ngeneralization performance.\\n\\nData Augmentation:\\nData augmentation involves artificially increasing the size and diversity of the training dataset by applying transformations\\nsuch as rotation, scaling, or flipping to the input data.\\nData augmentation helps expose the model to a wider range of variations in the data, making it more robust to different \\nscenarios and reducing overfitting.\\nBy incorporating regularization techniques into the training process, machine learning practitioners can effectively prevent\\noverfitting and develop models that generalize well to new, unseen data. The choice of regularization technique and its\\nhyperparameters depends on the specific characteristics of the dataset and the model architecture.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans7=\"\"\"Regularization in machine learning is a set of techniques used to prevent overfitting by adding a penalty term to the\n",
    "model's loss function. The penalty term encourages the model to favor simpler solutions and reduces its tendency to fit noise\n",
    "in the training data. Regularization helps improve the model's generalization ability, allowing it to perform well on unseen \n",
    "data.\n",
    "\n",
    "Here are some common regularization techniques and how they work:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "L1 regularization adds a penalty term proportional to the absolute value of the model's coefficients to the loss function.\n",
    "The penalty term encourages sparsity in the model, leading to feature selection by shrinking some coefficients to zero.\n",
    "L1 regularization is effective for feature selection and building sparse models.\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "L2 regularization adds a penalty term proportional to the square of the model's coefficients to the loss function.\n",
    "The penalty term penalizes large coefficients, effectively shrinking them towards zero.\n",
    "L2 regularization encourages the model to distribute its weight more evenly across features, reducing the impact of individual \n",
    "features and preventing overfitting.\n",
    "\n",
    "Early Stopping:\n",
    "Early stopping is a simple regularization technique that halts the training process when the model's performance on a \n",
    "validation set starts to degrade.\n",
    "By preventing the model from training for too long and overfitting to the training data, early stopping helps improve \n",
    "generalization performance.\n",
    "\n",
    "Data Augmentation:\n",
    "Data augmentation involves artificially increasing the size and diversity of the training dataset by applying transformations\n",
    "such as rotation, scaling, or flipping to the input data.\n",
    "Data augmentation helps expose the model to a wider range of variations in the data, making it more robust to different \n",
    "scenarios and reducing overfitting.\n",
    "By incorporating regularization techniques into the training process, machine learning practitioners can effectively prevent\n",
    "overfitting and develop models that generalize well to new, unseen data. The choice of regularization technique and its\n",
    "hyperparameters depends on the specific characteristics of the dataset and the model architecture.\"\"\"\n",
    "Ans7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457cb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
